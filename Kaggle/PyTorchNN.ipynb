{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b705cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 115)\n",
    "pd.set_option('display.width', 250)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7881ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRaw = pd.read_csv('train_final.csv')\n",
    "trainData = trainRaw\n",
    "#Remove Duplicate Inputs in Training Data\n",
    "#trainData = trainRaw.drop_duplicates(subset=trainRaw.columns.difference(['income>50K']),keep=False).reset_index(drop=True)\n",
    "x = len(trainData.index)\n",
    "\n",
    "testData = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42fa5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore Training Data with ?\n",
    "trainData = trainData.replace(to_replace = \"?\",value=np.nan)\n",
    "trainData = trainData.dropna().reset_index(drop=True)\n",
    "x = len(trainData.index)\n",
    "data = trainData.append(testData.iloc[:,1:],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d433d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing data with mode of column\n",
    "data = trainData.append(testData.iloc[:,1:],ignore_index=True)\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].replace(to_replace=\"?\",value=data[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1476a35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Replace missing data with mode matching output\n",
    "for row in range(len(trainData.index)):\n",
    "    for col in range(len(trainData.columns)):\n",
    "        if(trainData.iloc[row,col] == \"?\"):\n",
    "            subTrainData = trainData[trainData['income>50K'] == trainData.iloc[row,-1]]\n",
    "            mode = subTrainData[trainData.columns[col]].mode()[0]\n",
    "            trainData.iloc[row,col] = mode\n",
    "\n",
    "data = trainData.append(testData.iloc[:,1:],ignore_index=True)\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].replace(to_replace=\"?\",value=data[col].mode()[0])\n",
    "    \n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "80f91273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop fnlwgt and eduction.num, the former because random, the latter because redundant\n",
    "data = data.drop(['fnlwgt','education.num'],axis=1)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1b61099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48842\n",
      "(25000, 103)\n",
      "25000\n",
      "23842\n"
     ]
    }
   ],
   "source": [
    "datDummies = pd.get_dummies(data.iloc[:,:-1])\n",
    "datDummies = (datDummies-datDummies.min())/(datDummies.max()-datDummies.min())\n",
    "#print(datDummies)\n",
    "\n",
    "X = datDummies.iloc[:x,:]\n",
    "y = data['income>50K'][:x]\n",
    "test = datDummies.iloc[x:,:]\n",
    "print(len(datDummies.index))\n",
    "print(X.shape)\n",
    "N = X.shape[1]\n",
    "print(len(y.index))\n",
    "print(len(test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0eab18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1554012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "train_data = TrainData(torch.FloatTensor(X.to_numpy()), torch.FloatTensor(y.to_numpy()))\n",
    "\n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestData(torch.FloatTensor(test.to_numpy()))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7e485450",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self,N):\n",
    "        super(BinaryClassification, self).__init__()        \n",
    "        # Number of input features is 105\n",
    "        self.layer_1 = nn.Linear(N, 16) \n",
    "        self.layer_2 = nn.Linear(16,16)\n",
    "        self.layer_out = nn.Linear(16, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(16)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c918621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc50ffda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassification(\n",
      "  (layer_1): Linear(in_features=103, out_features=16, bias=True)\n",
      "  (layer_2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BinaryClassification(N)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "lambda1 = lambda epoch: 1/(1+epoch/30)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "13545d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7a06ce70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: | Loss: 0.39538 | Acc: 81.520\n",
      "Epoch 00002: | Loss: 0.33605 | Acc: 84.440\n",
      "Epoch 00003: | Loss: 0.33205 | Acc: 84.496\n",
      "Epoch 00004: | Loss: 0.32644 | Acc: 84.896\n",
      "Epoch 00005: | Loss: 0.32544 | Acc: 84.800\n",
      "Epoch 00006: | Loss: 0.31960 | Acc: 85.088\n",
      "Epoch 00007: | Loss: 0.31582 | Acc: 85.248\n",
      "Epoch 00008: | Loss: 0.31439 | Acc: 85.528\n",
      "Epoch 00009: | Loss: 0.31078 | Acc: 85.640\n",
      "Epoch 00010: | Loss: 0.30998 | Acc: 85.472\n",
      "Epoch 00011: | Loss: 0.30624 | Acc: 85.664\n",
      "Epoch 00012: | Loss: 0.30678 | Acc: 85.632\n",
      "Epoch 00013: | Loss: 0.30454 | Acc: 85.856\n",
      "Epoch 00014: | Loss: 0.30458 | Acc: 85.752\n",
      "Epoch 00015: | Loss: 0.30351 | Acc: 85.640\n",
      "Epoch 00016: | Loss: 0.30337 | Acc: 85.832\n",
      "Epoch 00017: | Loss: 0.29970 | Acc: 85.832\n",
      "Epoch 00018: | Loss: 0.29891 | Acc: 85.904\n",
      "Epoch 00019: | Loss: 0.30038 | Acc: 86.064\n",
      "Epoch 00020: | Loss: 0.29790 | Acc: 86.272\n",
      "Epoch 00021: | Loss: 0.29660 | Acc: 86.152\n",
      "Epoch 00022: | Loss: 0.29807 | Acc: 86.112\n",
      "Epoch 00023: | Loss: 0.29411 | Acc: 86.272\n",
      "Epoch 00024: | Loss: 0.29479 | Acc: 86.240\n",
      "Epoch 00025: | Loss: 0.29343 | Acc: 86.392\n",
      "Epoch 00026: | Loss: 0.29399 | Acc: 86.184\n",
      "Epoch 00027: | Loss: 0.29237 | Acc: 86.288\n",
      "Epoch 00028: | Loss: 0.29300 | Acc: 86.352\n",
      "Epoch 00029: | Loss: 0.29283 | Acc: 86.344\n",
      "Epoch 00030: | Loss: 0.29131 | Acc: 86.352\n",
      "Epoch 00031: | Loss: 0.29200 | Acc: 86.344\n",
      "Epoch 00032: | Loss: 0.29204 | Acc: 86.352\n",
      "Epoch 00033: | Loss: 0.28884 | Acc: 86.536\n",
      "Epoch 00034: | Loss: 0.28952 | Acc: 86.392\n",
      "Epoch 00035: | Loss: 0.28990 | Acc: 86.280\n",
      "Epoch 00036: | Loss: 0.28856 | Acc: 86.568\n",
      "Epoch 00037: | Loss: 0.28768 | Acc: 86.528\n",
      "Epoch 00038: | Loss: 0.28807 | Acc: 86.672\n",
      "Epoch 00039: | Loss: 0.28957 | Acc: 86.488\n",
      "Epoch 00040: | Loss: 0.28612 | Acc: 86.736\n",
      "Epoch 00041: | Loss: 0.28640 | Acc: 86.528\n",
      "Epoch 00042: | Loss: 0.28624 | Acc: 86.672\n",
      "Epoch 00043: | Loss: 0.28771 | Acc: 86.720\n",
      "Epoch 00044: | Loss: 0.28534 | Acc: 86.520\n",
      "Epoch 00045: | Loss: 0.28545 | Acc: 86.544\n",
      "Epoch 00046: | Loss: 0.28537 | Acc: 86.624\n",
      "Epoch 00047: | Loss: 0.28517 | Acc: 86.624\n",
      "Epoch 00048: | Loss: 0.28497 | Acc: 86.776\n",
      "Epoch 00049: | Loss: 0.28540 | Acc: 86.560\n",
      "Epoch 00050: | Loss: 0.28449 | Acc: 86.696\n",
      "Epoch 00051: | Loss: 0.28374 | Acc: 86.736\n",
      "Epoch 00052: | Loss: 0.28293 | Acc: 86.712\n",
      "Epoch 00053: | Loss: 0.28238 | Acc: 86.808\n",
      "Epoch 00054: | Loss: 0.28157 | Acc: 86.840\n",
      "Epoch 00055: | Loss: 0.28179 | Acc: 86.832\n",
      "Epoch 00056: | Loss: 0.28277 | Acc: 86.784\n",
      "Epoch 00057: | Loss: 0.28097 | Acc: 86.888\n",
      "Epoch 00058: | Loss: 0.28466 | Acc: 86.728\n",
      "Epoch 00059: | Loss: 0.28496 | Acc: 86.680\n",
      "Epoch 00060: | Loss: 0.28103 | Acc: 86.816\n",
      "Epoch 00061: | Loss: 0.28354 | Acc: 86.672\n",
      "Epoch 00062: | Loss: 0.28235 | Acc: 87.000\n",
      "Epoch 00063: | Loss: 0.28299 | Acc: 86.928\n",
      "Epoch 00064: | Loss: 0.28097 | Acc: 86.840\n",
      "Epoch 00065: | Loss: 0.28369 | Acc: 86.744\n",
      "Epoch 00066: | Loss: 0.28144 | Acc: 86.984\n",
      "Epoch 00067: | Loss: 0.28138 | Acc: 86.872\n",
      "Epoch 00068: | Loss: 0.28223 | Acc: 86.856\n",
      "Epoch 00069: | Loss: 0.27876 | Acc: 86.960\n",
      "Epoch 00070: | Loss: 0.28035 | Acc: 86.944\n",
      "Epoch 00071: | Loss: 0.27985 | Acc: 86.912\n",
      "Epoch 00072: | Loss: 0.27938 | Acc: 87.208\n",
      "Epoch 00073: | Loss: 0.28041 | Acc: 86.792\n",
      "Epoch 00074: | Loss: 0.27982 | Acc: 86.848\n",
      "Epoch 00075: | Loss: 0.28000 | Acc: 87.096\n",
      "Epoch 00076: | Loss: 0.28006 | Acc: 86.888\n",
      "Epoch 00077: | Loss: 0.28077 | Acc: 86.840\n",
      "Epoch 00078: | Loss: 0.27963 | Acc: 87.080\n",
      "Epoch 00079: | Loss: 0.27920 | Acc: 87.024\n",
      "Epoch 00080: | Loss: 0.27775 | Acc: 87.136\n",
      "Epoch 00081: | Loss: 0.27875 | Acc: 86.952\n",
      "Epoch 00082: | Loss: 0.27998 | Acc: 86.976\n",
      "Epoch 00083: | Loss: 0.27919 | Acc: 86.960\n",
      "Epoch 00084: | Loss: 0.28020 | Acc: 86.864\n",
      "Epoch 00085: | Loss: 0.27730 | Acc: 87.072\n",
      "Epoch 00086: | Loss: 0.27696 | Acc: 86.960\n",
      "Epoch 00087: | Loss: 0.27748 | Acc: 87.176\n",
      "Epoch 00088: | Loss: 0.27972 | Acc: 87.128\n",
      "Epoch 00089: | Loss: 0.27744 | Acc: 87.160\n",
      "Epoch 00090: | Loss: 0.27968 | Acc: 87.104\n",
      "Epoch 00091: | Loss: 0.27691 | Acc: 87.128\n",
      "Epoch 00092: | Loss: 0.27868 | Acc: 87.008\n",
      "Epoch 00093: | Loss: 0.27554 | Acc: 87.160\n",
      "Epoch 00094: | Loss: 0.27820 | Acc: 86.848\n",
      "Epoch 00095: | Loss: 0.27868 | Acc: 86.976\n",
      "Epoch 00096: | Loss: 0.27649 | Acc: 87.184\n",
      "Epoch 00097: | Loss: 0.27832 | Acc: 87.072\n",
      "Epoch 00098: | Loss: 0.27681 | Acc: 86.896\n",
      "Epoch 00099: | Loss: 0.27708 | Acc: 87.064\n",
      "Epoch 00100: | Loss: 0.27580 | Acc: 87.264\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:05}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "96a46810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 1., 0., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        \n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "y_pred_list = np.array(y_pred_list)\n",
    "y_pred_list = torch.from_numpy(y_pred_list)\n",
    "print(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64d88796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 1., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "valid = np.genfromtxt('validation.csv')\n",
    "valid = torch.from_numpy(valid)\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dad9793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78016\n"
     ]
    }
   ],
   "source": [
    "acc = binary_acc(y_pred_list,valid)\n",
    "\n",
    "print(f'{acc.item()/100*.91784:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29c9cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "with open('NNSub.csv', 'w') as f:\n",
    "    f.write(\"ID,Prediction\\n\")\n",
    "    for i in range(len(y_pred_list)):\n",
    "        f.write(str(i+1) + \",\" + str(y_pred_list[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd17ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
